{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom textwrap import wrap\nimport re\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nfrom peft import get_peft_model, LoraConfig\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:07:52.112388Z","iopub.execute_input":"2025-05-03T18:07:52.112751Z","iopub.status.idle":"2025-05-03T18:08:34.834215Z","shell.execute_reply.started":"2025-05-03T18:07:52.112718Z","shell.execute_reply":"2025-05-03T18:08:34.833531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = '/kaggle/input/flickr8k/Images'\ndf = pd.read_csv('/kaggle/input/flickr8k/captions.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:08:34.835566Z","iopub.execute_input":"2025-05-03T18:08:34.836192Z","iopub.status.idle":"2025-05-03T18:08:34.947743Z","shell.execute_reply.started":"2025-05-03T18:08:34.836168Z","shell.execute_reply":"2025-05-03T18:08:34.947098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:08:34.948582Z","iopub.execute_input":"2025-05-03T18:08:34.948842Z","iopub.status.idle":"2025-05-03T18:08:34.975501Z","shell.execute_reply.started":"2025-05-03T18:08:34.948823Z","shell.execute_reply":"2025-05-03T18:08:34.974811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def readImage(path,img_size=224):\n    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = img/255.\n    \n    return img\n    \ndef display_images(temp_df):\n    temp_df = temp_df.reset_index(drop=True)\n    plt.figure(figsize = (20 , 20))\n    n = 0\n    for i in range(temp_df.shape[0]):\n        n+=1\n        plt.subplot(5 , 5, n)\n        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n        image = readImage(f\"{image_path}/{temp_df.image[i]}\")\n        plt.imshow(image)\n        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n        plt.axis(\"off\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:08:34.976386Z","iopub.execute_input":"2025-05-03T18:08:34.976705Z","iopub.status.idle":"2025-05-03T18:08:34.983102Z","shell.execute_reply.started":"2025-05-03T18:08:34.976683Z","shell.execute_reply":"2025-05-03T18:08:34.982135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display_images(df.sample(15))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:08:34.985355Z","iopub.execute_input":"2025-05-03T18:08:34.985607Z","iopub.status.idle":"2025-05-03T18:08:36.890301Z","shell.execute_reply.started":"2025-05-03T18:08:34.985587Z","shell.execute_reply":"2025-05-03T18:08:36.889292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_caption(caption):\n    # Convert to lowercase\n    caption = caption.lower()\n    \n    # Remove punctuation and special characters except basic ones\n    caption = re.sub(r\"[^a-z0-9\\s]\", \"\", caption)\n    \n    # Remove extra spaces\n    caption = re.sub(r\"\\s+\", \" \", caption).strip()\n    \n    # Add start and end tokens\n    caption = \"<start> \" + caption + \" <end>\"\n    \n    return caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:08:36.891107Z","iopub.execute_input":"2025-05-03T18:08:36.891378Z","iopub.status.idle":"2025-05-03T18:08:36.897265Z","shell.execute_reply.started":"2025-05-03T18:08:36.891357Z","shell.execute_reply":"2025-05-03T18:08:36.896367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['caption'] = df['caption'].apply(preprocess_caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:08:36.898443Z","iopub.execute_input":"2025-05-03T18:08:36.898769Z","iopub.status.idle":"2025-05-03T18:08:37.200895Z","shell.execute_reply.started":"2025-05-03T18:08:36.898745Z","shell.execute_reply":"2025-05-03T18:08:37.200110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Image preprocessing\nimage_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:08:37.201819Z","iopub.execute_input":"2025-05-03T18:08:37.202154Z","iopub.status.idle":"2025-05-03T18:08:37.206998Z","shell.execute_reply.started":"2025-05-03T18:08:37.202128Z","shell.execute_reply":"2025-05-03T18:08:37.206207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset class\nclass Flickr8kDataset(Dataset):\n    def __init__(self, dataframe, image_folder, processor):\n        self.df = dataframe\n        self.image_folder = image_folder\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(os.path.join(self.image_folder, row[\"image\"])).convert(\"RGB\")\n        caption = row[\"caption\"]\n        proc_out = self.processor(\n            images=image,\n            return_tensors=\"pt\",\n            padding=\"max_length\"\n        )\n\n        proc_out = {\n            k: v.squeeze() for k, v in proc_out.items()\n        }\n        proc_out[\"text\"] = caption\n        return proc_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:08:37.207968Z","iopub.execute_input":"2025-05-03T18:08:37.208391Z","iopub.status.idle":"2025-05-03T18:08:37.227694Z","shell.execute_reply.started":"2025-05-03T18:08:37.208360Z","shell.execute_reply":"2025-05-03T18:08:37.226789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    # pad the input_ids and attention_mask\n    processed_batch = {}\n    for key in batch[0].keys():\n        if key != \"text\":\n            processed_batch[key] = torch.stack([example[key] for example in batch])\n        else:\n            text_inputs = processor.tokenizer(\n                [example[\"text\"] for example in batch], padding=True, return_tensors=\"pt\"\n            )\n            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n    return processed_batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:08:37.228764Z","iopub.execute_input":"2025-05-03T18:08:37.229146Z","iopub.status.idle":"2025-05-03T18:08:37.253211Z","shell.execute_reply.started":"2025-05-03T18:08:37.229119Z","shell.execute_reply":"2025-05-03T18:08:37.252343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# quant_config = BitsAndBytesConfig(load_in_8bit=True)\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"ybelkada/blip2-opt-2.7b-fp16-sharded\", \n    device_map=\"auto\", \n    # quantization_config=quant_config\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:08:37.254208Z","iopub.execute_input":"2025-05-03T18:08:37.254854Z","iopub.status.idle":"2025-05-03T18:09:15.518296Z","shell.execute_reply.started":"2025-05-03T18:08:37.254826Z","shell.execute_reply":"2025-05-03T18:09:15.517492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's define the LoraConfig\nconfig = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"k_proj\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:09:15.519231Z","iopub.execute_input":"2025-05-03T18:09:15.520081Z","iopub.status.idle":"2025-05-03T18:09:15.523701Z","shell.execute_reply.started":"2025-05-03T18:09:15.520029Z","shell.execute_reply":"2025-05-03T18:09:15.523112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = get_peft_model(model, config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:09:15.524453Z","iopub.execute_input":"2025-05-03T18:09:15.525007Z","iopub.status.idle":"2025-05-03T18:09:18.416948Z","shell.execute_reply.started":"2025-05-03T18:09:15.524982Z","shell.execute_reply":"2025-05-03T18:09:18.416247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = Flickr8kDataset(df, \"/kaggle/input/flickr8k/Images\", processor)\ntrain_size = int(0.8 * len(dataset))\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:09:18.419464Z","iopub.execute_input":"2025-05-03T18:09:18.419707Z","iopub.status.idle":"2025-05-03T18:09:18.429544Z","shell.execute_reply.started":"2025-05-03T18:09:18.419691Z","shell.execute_reply":"2025-05-03T18:09:18.428796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=5, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:09:18.430341Z","iopub.execute_input":"2025-05-03T18:09:18.430566Z","iopub.status.idle":"2025-05-03T18:09:18.442423Z","shell.execute_reply.started":"2025-05-03T18:09:18.430550Z","shell.execute_reply":"2025-05-03T18:09:18.441684Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:09:18.443238Z","iopub.execute_input":"2025-05-03T18:09:18.443564Z","iopub.status.idle":"2025-05-03T18:09:18.479481Z","shell.execute_reply.started":"2025-05-03T18:09:18.443538Z","shell.execute_reply":"2025-05-03T18:09:18.478833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:09:18.480400Z","iopub.execute_input":"2025-05-03T18:09:18.480695Z","iopub.status.idle":"2025-05-03T18:09:18.484219Z","shell.execute_reply.started":"2025-05-03T18:09:18.480665Z","shell.execute_reply":"2025-05-03T18:09:18.483419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    \n    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Training\", leave=False)\n\n    for idx, batch in progress_bar:\n        input_ids = batch.pop(\"input_ids\").to(device)\n        pixel_values = batch.pop(\"pixel_values\").to(device, torch.float16)\n\n        outputs = model(input_ids=input_ids,\n                        pixel_values=pixel_values,\n                        labels=input_ids)\n        \n        loss = outputs.loss\n\n        # Update progress bar description with current loss\n        progress_bar.set_postfix({\"loss\": loss.item()})\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:09:18.484951Z","iopub.execute_input":"2025-05-03T18:09:18.485159Z","iopub.status.idle":"2025-05-03T21:27:32.187694Z","shell.execute_reply.started":"2025-05-03T18:09:18.485144Z","shell.execute_reply":"2025-05-03T21:27:32.186827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_DIR = \"blip2-opt2.7b-finetuned-lora\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:27:44.824842Z","iopub.execute_input":"2025-05-03T21:27:44.825414Z","iopub.status.idle":"2025-05-03T21:27:44.829106Z","shell.execute_reply.started":"2025-05-03T21:27:44.825390Z","shell.execute_reply":"2025-05-03T21:27:44.828360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(MODEL_DIR)\nprocessor.save_pretrained(MODEL_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T21:27:46.908076Z","iopub.execute_input":"2025-05-03T21:27:46.908674Z","iopub.status.idle":"2025-05-03T21:27:47.538153Z","shell.execute_reply.started":"2025-05-03T21:27:46.908648Z","shell.execute_reply":"2025-05-03T21:27:47.537435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N_QUAL = 20\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decoding strategies as per assignment\nDECODING_STRATEGIES = [\n    (\"beam\", {\"num_beams\": 5, \"max_new_tokens\": 30}),\n    (\"top_k\", {\"do_sample\": True, \"top_k\": 50, \"max_new_tokens\": 30}),\n    (\"top_p\", {\"do_sample\": True, \"top_p\": 0.9, \"max_new_tokens\": 30}),\n    (\"temperature\", {\"do_sample\": True, \"temperature\": 0.7, \"max_new_tokens\": 30}),\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model and processor\ndef load_model_and_processor():\n    try:\n        processor = AutoProcessor.from_pretrained(MODEL_DIR)\n        model = Blip2ForConditionalGeneration.from_pretrained(MODEL_DIR, torch_dtype=torch.float16)\n        model = model.to(DEVICE)\n        model.eval()\n        return processor, model\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load model or processor: {e}\")\n\nprocessor, model = load_model_and_processor()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generative decoding function\ndef generate_caption(image, strategy, params):\n    \"\"\"Generate a caption with the given decoding strategy.\"\"\"\n    try:\n        inputs = processor(images=image, return_tensors=\"pt\").to(DEVICE, torch.float16)\n        outputs = model.generate(**inputs, **params)\n        caption = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return caption.strip()\n    except Exception as e:\n        print(f\"Error generating caption for strategy {strategy}: {e}\")\n        return \"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Automatic metrics setup\nbleu_metric = evaluate.load(\"bleu\")\nmeteor_metric = evaluate.load(\"meteor\")\nrouge_metric = evaluate.load(\"rouge\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_self_bleu(captions):\n    \"\"\"Compute Self-BLEU to measure diversity (lower is more diverse).\"\"\"\n    scores = []\n    for i, hyp in enumerate(captions):\n        refs = [captions[j] for j in range(len(captions)) if j != i]\n        score = bleu_metric.compute(predictions=[hyp], references=[refs], max_order=4)[\"bleu\"]\n        scores.append(score)\n    return np.mean(scores) if scores else 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_distinct_n(captions, n=2):\n    \"\"\"Compute Distinct-n to measure diversity (higher is more diverse).\"\"\"\n    ngrams = set()\n    total_ngrams = 0\n    for caption in captions:\n        tokens = word_tokenize(caption.lower())\n        for i in range(len(tokens) - n + 1):\n            ngram = tuple(tokens[i:i+n])\n            ngrams.add(ngram)\n            total_ngrams += 1\n    return len(ngrams) / total_ngrams if total_ngrams > 0 else 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(refs, hyps):\n    \"\"\"Compute BLEU-4, METEOR, ROUGE-L, Self-BLEU, Distinct-n (SPICE placeholder).\"\"\"\n    try:\n        # Handle multiple references per image (Flickr8k has 5 captions per image)\n        bleu_score = bleu_metric.compute(predictions=hyps, references=refs, max_order=4)[\"bleu\"]\n        meteor_score = meteor_metric.compute(predictions=hyps, references=[r[0] for r in refs])[\"meteor\"]\n        rouge_score = rouge_metric.compute(predictions=hyps, references=[r[0] for r in refs])[\"rougeL\"]\n        self_bleu = compute_self_bleu(hyps)\n        distinct_2 = compute_distinct_n(hyps, n=2)\n        # SPICE requires external setup; use placeholder (implement if pycocoevalcap is available)\n        spice_score = 0.0  # Placeholder\n        return {\n            \"BLEU-4\": bleu_score,\n            \"METEOR\": meteor_score,\n            \"ROUGE-L\": rouge_score,\n            \"Self-BLEU\": self_bleu,\n            \"Distinct-2\": distinct_2,\n            \"SPICE\": spice_score\n        }\n    except Exception as e:\n        print(f\"Error computing metrics: {e}\")\n        return {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run automatic evaluation on validation set\ndef evaluate_dataset(df, output_file=\"metrics_results.csv\"):\n    \"\"\"Evaluate all decoding strategies on the validation set and save results.\"\"\"\n    results = []\n    for strat, params in DECODING_STRATEGIES:\n        all_refs, all_hyps = [], []\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Evaluating {strat}\"):\n            img_path = os.path.join(IMAGE_DIR, row[\"image\"])\n            if not os.path.exists(img_path):\n                print(f\"Image not found: {img_path}\")\n                continue\n            img = Image.open(img_path).convert(\"RGB\")\n            # Flickr8k has 5 captions; use all for BLEU, first for others\n            refs = df[df[\"image\"] == row[\"image\"]][\"caption\"].tolist()\n            hyp = generate_caption(img, strat, params)\n            all_refs.append(refs)\n            all_hyps.append(hyp)\n        metrics = compute_metrics(all_refs, all_hyps)\n        metrics[\"strategy\"] = strat\n        results.append(metrics)\n        print(f\"Metrics for {strat}: {metrics}\")\n    \n    # Save metrics to CSV\n    with open(output_file, \"w\", newline=\"\") as f:\n        fieldnames = [\"strategy\", \"BLEU-4\", \"METEOR\", \"ROUGE-L\", \"Self-BLEU\", \"Distinct-2\", \"SPICE\"]\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(results)\n    print(f\"Metrics saved to {output_file}\")\n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Qualitative and error analysis\ndef qualitative_analysis(df, output_csv=\"qualitative_analysis.csv\"):\n    \"\"\"Select N_QUAL samples, generate captions, detect errors, and save to CSV.\"\"\"\n    df_sample = df.sample(N_QUAL, random_state=42).reset_index(drop=True)\n    rows = []\n    key_elements_count = 0\n    hallucination_free_count = 0\n    \n    for _, row in df_sample.iterrows():\n        img_path = os.path.join(IMAGE_DIR, row[\"image\"])\n        if not os.path.exists(img_path):\n            print(f\"Image not found: {img_path}\")\n            continue\n        img = Image.open(img_path).convert(\"RGB\")\n        refs = df[df[\"image\"] == row[\"image\"]][\"caption\"].tolist()\n        ref_tokens = set(word_tokenize(\" \".join(refs).lower()))\n        \n        for strat, params in DECODING_STRATEGIES:\n            gen = generate_caption(img, strat, params)\n            gen_tokens = word_tokenize(gen.lower())\n            issue = []\n            \n            # Hallucination: tokens not in any reference (excluding common words)\n            stop_words = set(nltk.corpus.stopwords.words('english'))\n            if any(tok not in ref_tokens and tok not in stop_words for tok in gen_tokens):\n                issue.append(\"hallucination\")\n            else:\n                hallucination_free_count += 1\n            \n            # Repetition: same word appearing multiple times\n            word_counts = {tok: gen_tokens.count(tok) for tok in set(gen_tokens)}\n            if any(count > 2 for count in word_counts.values()):\n                issue.append(\"repetition\")\n            \n            # Omission: significantly shorter than average reference length\n            avg_ref_len = np.mean([len(word_tokenize(r)) for r in refs])\n            if len(gen_tokens) < avg_ref_len / 2:\n                issue.append(\"omission\")\n            \n            # Key elements: count unique nouns as proxy (requires NLTK pos_tag)\n            pos_tags = nltk.pos_tag(gen_tokens)\n            nouns = len([t for t, pos in pos_tags if pos.startswith('NN')])\n            if nouns >= 3:\n                key_elements_count += 1\n            \n            rows.append({\n                \"image\": row[\"image\"],\n                \"strategy\": strat,\n                \"reference\": refs[0],\n                \"generated\": gen,\n                \"issue\": \"; \".join(issue) if issue else \"OK\",\n                \"noun_count\": nouns\n            })\n    \n    # Save to CSV\n    with open(output_csv, \"w\", newline=\"\") as f:\n        fieldnames = [\"image\", \"strategy\", \"reference\", \"generated\", \"issue\", \"noun_count\"]\n        writer = csv.DictWriter(f, fieldnames=fieldnames)\n        writer.writeheader()\n        writer.writerows(rows)\n    \n    # Success criteria check\n    total_samples = len(df_sample) * len(DECODING_STRATEGIES)\n    print(f\"Qualitative analysis saved to {output_csv}\")\n    print(f\"Descriptions with â‰¥3 key elements: {key_elements_count/total_samples*100:.2f}%\")\n    print(f\"Hallucination-free descriptions: {hallucination_free_count/total_samples*100:.2f}%\")\n    return rows","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage (assumes val_df is defined as a DataFrame with 'image' and 'caption' columns)\n# metrics = evaluate_dataset(val_df)\n# qualitative_rows = qualitative_analysis(val_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}